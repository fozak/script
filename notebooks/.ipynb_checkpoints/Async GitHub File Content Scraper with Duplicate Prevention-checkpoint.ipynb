{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfd897a-01c6-49e8-87a1-7d0e8e25a3bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Run the async main function\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\python\\Lib\\asyncio\\runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# WORKS BUT NEEDS DELAY BETEWEEN REQUESTS\n",
    "# could be easier to fork and to seach locally\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# GitHub API base\n",
    "API_BASE_URL = \"https://api.github.com/repos/frappe/lms/contents\"\n",
    "BRANCH = \"dfb82570ea77c74e5e0d905ff92c709568cb0877\"\n",
    "RAW_BASE_URL = \"https://raw.githubusercontent.com/frappe/lms\"\n",
    "OUTPUT_FILE = \"all_files.txt\"\n",
    "\n",
    "# Track processed files to avoid duplicates\n",
    "processed_files = set()\n",
    "\n",
    "async def get_files_recursive(session, path=\"lms/lms/doctype\", processed_dirs=None):\n",
    "    \"\"\"Recursively fetch all files in a GitHub directory.\"\"\"\n",
    "    if processed_dirs is None:\n",
    "        processed_dirs = set()\n",
    "    \n",
    "    # Skip if we've already processed this directory\n",
    "    if path in processed_dirs:\n",
    "        print(f\"‚è© Skipping already processed directory: {path}\")\n",
    "        return []\n",
    "    \n",
    "    # Mark this directory as processed\n",
    "    processed_dirs.add(path)\n",
    "    \n",
    "    url = f\"{API_BASE_URL}/{path}?ref={BRANCH}\"\n",
    "    \n",
    "    headers = {}  # Consider adding your GitHub token here for higher rate limits\n",
    "    # headers = {\"Authorization\": \"token YOUR_GITHUB_TOKEN\"}\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"‚ùå Failed to fetch directory listing: {path}\")\n",
    "                print(f\"Status code: {response.status}\")\n",
    "                error_text = await response.text()\n",
    "                print(f\"Response: {error_text}\")\n",
    "                return []\n",
    "            \n",
    "            file_data = await response.json()\n",
    "            files = []\n",
    "            \n",
    "            # Create tasks for subdirectory processing\n",
    "            subdir_tasks = []\n",
    "            \n",
    "            for item in file_data:\n",
    "                if item[\"type\"] == \"file\":\n",
    "                    file_path = item[\"path\"]\n",
    "                    # Check if we've already processed this file\n",
    "                    if file_path not in processed_files:\n",
    "                        processed_files.add(file_path)\n",
    "                        files.append(file_path)\n",
    "                    else:\n",
    "                        print(f\"‚è© Skipping duplicate file: {file_path}\")\n",
    "                elif item[\"type\"] == \"dir\":\n",
    "                    subdir_path = item[\"path\"]\n",
    "                    print(f\"üîç Exploring directory: {subdir_path}\")\n",
    "                    # Create a task to process this subdirectory\n",
    "                    task = asyncio.create_task(\n",
    "                        get_files_recursive(session, subdir_path, processed_dirs)\n",
    "                    )\n",
    "                    subdir_tasks.append(task)\n",
    "            \n",
    "            # Wait for all subdirectory tasks to complete\n",
    "            for task in asyncio.as_completed(subdir_tasks):\n",
    "                subdir_files = await task\n",
    "                files.extend(subdir_files)\n",
    "            \n",
    "            return files\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing directory {path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "async def download_file(session, file_path, sem):\n",
    "    \"\"\"Download a single file with rate limiting.\"\"\"\n",
    "    file_url = f\"{RAW_BASE_URL}/{BRANCH}/{file_path}\"\n",
    "    \n",
    "    async with sem:  # Use semaphore to limit concurrent requests\n",
    "        try:\n",
    "            async with session.get(file_url) as response:\n",
    "                if response.status == 200:\n",
    "                    content = await response.text()\n",
    "                    print(f\"‚úÖ Downloaded: {file_path}\")\n",
    "                    return file_path, content\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch: {file_path}\")\n",
    "                    print(f\"Status code: {response.status}\")\n",
    "                    print(f\"URL attempted: {file_url}\")\n",
    "                    return file_path, None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading {file_path}: {str(e)}\")\n",
    "            return file_path, None\n",
    "\n",
    "async def download_and_save(file_list):\n",
    "    \"\"\"Download files concurrently and append to a single text file with separators.\"\"\"\n",
    "    # Use a semaphore to limit the number of concurrent requests\n",
    "    # to avoid overwhelming the GitHub API\n",
    "    sem = asyncio.Semaphore(5)  # Max 5 concurrent requests\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create download tasks for all files\n",
    "        tasks = [download_file(session, file_path, sem) for file_path in file_list]\n",
    "        \n",
    "        # Open file once for writing all content\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for completed_task in asyncio.as_completed(tasks):\n",
    "                file_path, content = await completed_task\n",
    "                if content is not None:\n",
    "                    outfile.write(f\"\\\\{file_path}\\n\")  # Separator in the requested format\n",
    "                    outfile.write(content + \"\\n\\n\")  # Append content\n",
    "\n",
    "async def main():\n",
    "    # Create a single session for all requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Fetch file list (recursive)\n",
    "        print(\"üîç Starting to discover files...\")\n",
    "        file_list = await get_files_recursive(session)\n",
    "        \n",
    "        if file_list:\n",
    "            print(f\"üìã Found {len(file_list)} unique files. Starting download...\")\n",
    "            await download_and_save(file_list)\n",
    "            print(f\"\\n‚úÖ All files saved to {OUTPUT_FILE}\")\n",
    "            print(f\"Total unique files processed: {len(processed_files)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No files found.\")\n",
    "\n",
    "# Run the async main function\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0de35-e78e-40ec-8a2a-dcdb6cd00ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
