{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d12c341-5be0-4451-9d21-9ae917193fa4",
   "metadata": {},
   "source": [
    "https://claude.ai/chat/4fd9b8d3-454b-49ba-9857-0b8e8ecbc619\n",
    "\n",
    "Summary - Cell 2 given by claude, just gives you huge flat file of all classes and functions.\n",
    "\n",
    "IMPROVED Cell 3 - only particular module \n",
    "\n",
    "AI prompt for NEXT notebook\n",
    "is it possible to use changed logic, instead of parsing all repo (witch is big) allow user to input directlory of particular module (which is located in that directory) and parse it first and only related classes and functions that are references in that particular set of code (example of directory with module C:\\python\\erpnext\\erpnext\\projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9bbe2e-698b-4d37-8bfe-a7461dd8e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076ff96834114f279525df3ea6277820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Repository Path:', layout=Layout(width='50%'), placeholder='Enter the path to your…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing repository: C:\\python\\erpnext\\erpnext\n",
      "Processing: www\\support\\__init__.pyinit__.pyit__.pyyynit__.pyeractions.pys.pyngs.pyg_schedule.pyupplied_item.pyparison.pyem_group.pypy\n",
      "Finished processing repository. Map contains 1524 files.\n",
      "\n",
      "Repository map has been generated.\n",
      "AI-friendly format saved to: repo_map_ai.txt\n",
      "JSON format saved to: repo_map.json\n",
      "\n",
      "Preview of the AI-friendly format:\n",
      "setup\\setup_wizard\\operations\\install_fixtures.py:\n",
      "│def _(x, *args, **kwargs):\n",
      "│\t\"\"\"Redefine the translation function to return the string as is.\n",
      "│\n",
      "│\tWe want to create english records but still mark the strings as translatable.\n",
      "│\tThe respective DocTypes have 'Translate Link Fields' enabled.\"\"\"\n",
      "│\treturn x\n",
      "⋮...\n",
      "│def create_bank_account(args):\n",
      "⋮...\n",
      "│def add_uom_data():\n",
      "⋮...\n",
      "│def get_fy_details(fy_start_date, fy_end_date):\n",
      "⋮...\n",
      "│def read_lines(filename: str) -> list[str]:\n",
      "│\t\"\"\"Return a list of lines from a file in the data directory.\"\"\"\n",
      "│\treturn (Path(__file__).parent.parent / \"data\" / filename).read_text().splitlines()\n",
      "⋮...\n",
      "│def update_selling_defaults():\n",
      "⋮...\n",
      "... (more content in repo_map_ai.txt)\n"
     ]
    }
   ],
   "source": [
    "# Python Repository Map Generator for AI Processing\n",
    "# Inspired by aider's approach to create concise code context for LLMs\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import sys\n",
    "import importlib.util\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "class RepoMapGenerator:\n",
    "    def __init__(self, root_dir, max_tokens=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.max_tokens = max_tokens\n",
    "        self.files_data = []\n",
    "        self.symbol_references = defaultdict(int)\n",
    "        self.file_dependencies = defaultdict(set)\n",
    "        self.symbol_definitions = {}  # filepath -> symbol -> line info\n",
    "        \n",
    "    def process_repository(self):\n",
    "        \"\"\"Process the entire repository to build the repo map\"\"\"\n",
    "        print(f\"Processing repository: {self.root_dir}\")\n",
    "        \n",
    "        # First pass: collect all symbol definitions and references\n",
    "        for root, _, files in os.walk(self.root_dir):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".py\"):\n",
    "                    filepath = os.path.join(root, filename)\n",
    "                    rel_path = os.path.relpath(filepath, self.root_dir)\n",
    "                    \n",
    "                    print(f\"Processing: {rel_path}\", end=\"\\r\")\n",
    "                    file_data = self.parse_python_file(filepath)\n",
    "                    \n",
    "                    if file_data:\n",
    "                        self.files_data.append(file_data)\n",
    "                        self.extract_symbols_and_references(file_data)\n",
    "        \n",
    "        # Second pass: compute importance scores\n",
    "        self.compute_symbol_importance()\n",
    "        \n",
    "        # Third pass: build the repo map\n",
    "        repo_map = self.build_repo_map()\n",
    "        \n",
    "        print(f\"\\nFinished processing repository. Map contains {len(repo_map)} files.\")\n",
    "        return repo_map\n",
    "    \n",
    "    def parse_python_file(self, filepath: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a Python file using AST\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                source_code = file.read()\n",
    "                source_lines = source_code.splitlines()\n",
    "            \n",
    "            # Parse the source code into an AST\n",
    "            tree = ast.parse(source_code)\n",
    "            \n",
    "            # Extract file information\n",
    "            rel_path = os.path.relpath(filepath, self.root_dir)\n",
    "            \n",
    "            file_data = {\n",
    "                \"filepath\": filepath,\n",
    "                \"rel_filepath\": rel_path,\n",
    "                \"source_lines\": source_lines,\n",
    "                \"symbols\": self.extract_symbols(tree, source_lines),\n",
    "                \"imports\": self.extract_imports(tree),\n",
    "                \"references\": self.extract_references(tree)\n",
    "            }\n",
    "            \n",
    "            return file_data\n",
    "        \n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in {filepath}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {filepath}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_symbols(self, tree: ast.AST, source_lines: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract symbol definitions from AST\"\"\"\n",
    "        symbols = []\n",
    "        \n",
    "        for node in ast.iter_child_nodes(tree):\n",
    "            if isinstance(node, ast.ClassDef):\n",
    "                # Extract class definition\n",
    "                class_symbol = {\n",
    "                    \"type\": \"class\",\n",
    "                    \"name\": node.name,\n",
    "                    \"line_start\": node.lineno,\n",
    "                    \"line_end\": getattr(node, 'end_lineno', node.lineno),\n",
    "                    \"code_text\": self.get_definition_text(source_lines, node),\n",
    "                    \"methods\": []\n",
    "                }\n",
    "                \n",
    "                # Extract class methods\n",
    "                for item in node.body:\n",
    "                    if isinstance(item, ast.FunctionDef):\n",
    "                        method_symbol = {\n",
    "                            \"type\": \"method\",\n",
    "                            \"name\": f\"{node.name}.{item.name}\",\n",
    "                            \"method_name\": item.name,\n",
    "                            \"line_start\": item.lineno,\n",
    "                            \"line_end\": getattr(item, 'end_lineno', item.lineno),\n",
    "                            \"code_text\": self.get_definition_text(source_lines, item),\n",
    "                            \"args\": self.get_function_args(item)\n",
    "                        }\n",
    "                        class_symbol[\"methods\"].append(method_symbol)\n",
    "                \n",
    "                symbols.append(class_symbol)\n",
    "            \n",
    "            elif isinstance(node, ast.FunctionDef):\n",
    "                # Extract function definition\n",
    "                func_symbol = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": node.name,\n",
    "                    \"line_start\": node.lineno,\n",
    "                    \"line_end\": getattr(node, 'end_lineno', node.lineno),\n",
    "                    \"code_text\": self.get_definition_text(source_lines, node),\n",
    "                    \"args\": self.get_function_args(node)\n",
    "                }\n",
    "                symbols.append(func_symbol)\n",
    "        \n",
    "        return symbols\n",
    "    \n",
    "    def get_definition_text(self, source_lines: List[str], node: Union[ast.ClassDef, ast.FunctionDef]) -> str:\n",
    "        \"\"\"Get the definition line(s) for a symbol\"\"\"\n",
    "        start_line = node.lineno - 1  # 0-indexed\n",
    "        \n",
    "        # For single-line functions or classes\n",
    "        if hasattr(node, 'end_lineno'):\n",
    "            if node.end_lineno - node.lineno <= 5:  # If definition is short (≤ 5 lines)\n",
    "                # Get full definition \n",
    "                return \"\\n\".join(source_lines[start_line:node.end_lineno])\n",
    "        \n",
    "        # For longer definitions or when end_lineno is not available\n",
    "        # Get just the signature/declaration\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            # For classes, include inheritance info\n",
    "            line = source_lines[start_line]\n",
    "            # If declaration continues to next lines, include them\n",
    "            current_line = start_line + 1\n",
    "            while current_line < len(source_lines) and ':' not in line:\n",
    "                line += \"\\n\" + source_lines[current_line]\n",
    "                current_line += 1\n",
    "            return line\n",
    "        \n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            # For functions, include the full signature\n",
    "            signature_lines = []\n",
    "            signature_lines.append(source_lines[start_line])\n",
    "            \n",
    "            # If parameter list continues to next lines\n",
    "            line = source_lines[start_line]\n",
    "            paren_count = line.count('(') - line.count(')')\n",
    "            current_line = start_line + 1\n",
    "            \n",
    "            # Keep adding lines until we have a balanced parenthesis count or hit a colon\n",
    "            while paren_count > 0 and current_line < len(source_lines):\n",
    "                next_line = source_lines[current_line]\n",
    "                signature_lines.append(next_line)\n",
    "                paren_count += next_line.count('(') - next_line.count(')')\n",
    "                \n",
    "                if ':' in next_line:\n",
    "                    break\n",
    "                    \n",
    "                current_line += 1\n",
    "            \n",
    "            return \"\\n\".join(signature_lines)\n",
    "    \n",
    "    def get_function_args(self, node: ast.FunctionDef) -> List[str]:\n",
    "        \"\"\"Extract function arguments\"\"\"\n",
    "        args = []\n",
    "        for arg in node.args.args:\n",
    "            arg_str = arg.arg\n",
    "            if arg.annotation:\n",
    "                if isinstance(arg.annotation, ast.Name):\n",
    "                    arg_str += f\": {arg.annotation.id}\"\n",
    "                elif isinstance(arg.annotation, ast.Attribute):\n",
    "                    arg_str += f\": {self.get_attribute_name(arg.annotation)}\"\n",
    "            args.append(arg_str)\n",
    "        \n",
    "        # Add *args if present\n",
    "        if node.args.vararg:\n",
    "            args.append(f\"*{node.args.vararg.arg}\")\n",
    "        \n",
    "        # Add **kwargs if present\n",
    "        if node.args.kwarg:\n",
    "            args.append(f\"**{node.args.kwarg.arg}\")\n",
    "            \n",
    "        return args\n",
    "    \n",
    "    def get_attribute_name(self, node: ast.Attribute) -> str:\n",
    "        \"\"\"Get the full name of an attribute node\"\"\"\n",
    "        if isinstance(node.value, ast.Name):\n",
    "            return f\"{node.value.id}.{node.attr}\"\n",
    "        elif isinstance(node.value, ast.Attribute):\n",
    "            return f\"{self.get_attribute_name(node.value)}.{node.attr}\"\n",
    "        return f\"?.{node.attr}\"\n",
    "    \n",
    "    def extract_imports(self, tree: ast.AST) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract import statements\"\"\"\n",
    "        imports = []\n",
    "        \n",
    "        for node in ast.iter_child_nodes(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"type\": \"import\",\n",
    "                        \"name\": name.name,\n",
    "                        \"asname\": name.asname\n",
    "                    })\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                module = node.module or \"\"\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"type\": \"import_from\",\n",
    "                        \"module\": module,\n",
    "                        \"name\": name.name,\n",
    "                        \"asname\": name.asname\n",
    "                    })\n",
    "        \n",
    "        return imports\n",
    "    \n",
    "    def extract_references(self, tree: ast.AST) -> List[str]:\n",
    "        \"\"\"Extract symbol references from AST\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        class ReferenceVisitor(ast.NodeVisitor):\n",
    "            def __init__(self):\n",
    "                self.refs = []\n",
    "                \n",
    "            def visit_Name(self, node):\n",
    "                if isinstance(node.ctx, ast.Load):\n",
    "                    self.refs.append(node.id)\n",
    "                self.generic_visit(node)\n",
    "                \n",
    "            def visit_Attribute(self, node):\n",
    "                if isinstance(node.value, ast.Name):\n",
    "                    self.refs.append(f\"{node.value.id}.{node.attr}\")\n",
    "                self.generic_visit(node)\n",
    "        \n",
    "        visitor = ReferenceVisitor()\n",
    "        visitor.visit(tree)\n",
    "        return visitor.refs\n",
    "    \n",
    "    def extract_symbols_and_references(self, file_data: Dict[str, Any]):\n",
    "        \"\"\"Extract symbols and references from file data\"\"\"\n",
    "        filepath = file_data[\"rel_filepath\"]\n",
    "        \n",
    "        # Store symbol definitions\n",
    "        self.symbol_definitions[filepath] = {}\n",
    "        for symbol in file_data[\"symbols\"]:\n",
    "            if symbol[\"type\"] == \"class\":\n",
    "                self.symbol_definitions[filepath][symbol[\"name\"]] = {\n",
    "                    \"type\": \"class\",\n",
    "                    \"line_start\": symbol[\"line_start\"],\n",
    "                    \"code_text\": symbol[\"code_text\"]\n",
    "                }\n",
    "                \n",
    "                # Add methods\n",
    "                for method in symbol[\"methods\"]:\n",
    "                    self.symbol_definitions[filepath][method[\"name\"]] = {\n",
    "                        \"type\": \"method\",\n",
    "                        \"line_start\": method[\"line_start\"],\n",
    "                        \"code_text\": method[\"code_text\"],\n",
    "                        \"args\": method[\"args\"]\n",
    "                    }\n",
    "            \n",
    "            elif symbol[\"type\"] == \"function\":\n",
    "                self.symbol_definitions[filepath][symbol[\"name\"]] = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"line_start\": symbol[\"line_start\"],\n",
    "                    \"code_text\": symbol[\"code_text\"],\n",
    "                    \"args\": symbol[\"args\"]\n",
    "                }\n",
    "        \n",
    "        # Count references\n",
    "        for ref in file_data[\"references\"]:\n",
    "            self.symbol_references[ref] += 1\n",
    "        \n",
    "        # Track file dependencies based on imports\n",
    "        for imp in file_data[\"imports\"]:\n",
    "            if imp[\"type\"] == \"import\":\n",
    "                self.file_dependencies[filepath].add(imp[\"name\"].split(\".\")[0])\n",
    "            elif imp[\"type\"] == \"import_from\":\n",
    "                self.file_dependencies[filepath].add(imp[\"module\"].split(\".\")[0])\n",
    "    \n",
    "    def compute_symbol_importance(self):\n",
    "        \"\"\"Compute importance scores for symbols\"\"\"\n",
    "        self.symbol_importance = {}\n",
    "        \n",
    "        # Start with reference counts as base importance\n",
    "        for symbol, count in self.symbol_references.items():\n",
    "            self.symbol_importance[symbol] = count\n",
    "        \n",
    "        # Add importance based on dependency graph - symbols in more depended-upon files\n",
    "        # are more important\n",
    "        dependency_counts = Counter()\n",
    "        for file, deps in self.file_dependencies.items():\n",
    "            for dep in deps:\n",
    "                dependency_counts[dep] += 1\n",
    "        \n",
    "        # Find Python files that match dependency names\n",
    "        for root, _, files in os.walk(self.root_dir):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".py\"):\n",
    "                    basename = os.path.splitext(filename)[0]\n",
    "                    if basename in dependency_counts:\n",
    "                        filepath = os.path.relpath(os.path.join(root, filename), self.root_dir)\n",
    "                        \n",
    "                        # Boost importance of symbols in this file\n",
    "                        if filepath in self.symbol_definitions:\n",
    "                            for symbol in self.symbol_definitions[filepath]:\n",
    "                                if symbol in self.symbol_importance:\n",
    "                                    self.symbol_importance[symbol] += dependency_counts[basename]\n",
    "                                else:\n",
    "                                    self.symbol_importance[symbol] = dependency_counts[basename]\n",
    "    \n",
    "    def estimate_token_count(self, text):\n",
    "        \"\"\"Estimate token count based on words and punctuation\"\"\"\n",
    "        # Simple token count estimation based on GPT tokenization heuristics\n",
    "        # This is a rough approximation\n",
    "        return len(re.findall(r'\\w+|[^\\w\\s]', text))\n",
    "    \n",
    "    def build_repo_map(self):\n",
    "        \"\"\"Build the repository map optimized for token count\"\"\"\n",
    "        repo_map = {}\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # Sort files by their dependency count (most depended upon first)\n",
    "        file_importance = {}\n",
    "        for filepath in self.symbol_definitions:\n",
    "            # Calculate file importance based on symbols it contains\n",
    "            importance = sum(\n",
    "                self.symbol_importance.get(symbol, 0)\n",
    "                for symbol in self.symbol_definitions[filepath]\n",
    "            )\n",
    "            file_importance[filepath] = importance\n",
    "        \n",
    "        sorted_files = sorted(file_importance.keys(), key=lambda f: file_importance[f], reverse=True)\n",
    "        \n",
    "        # Build map with the most important files first\n",
    "        for filepath in sorted_files:\n",
    "            file_symbols = self.symbol_definitions[filepath]\n",
    "            \n",
    "            # Sort symbols by importance\n",
    "            sorted_symbols = sorted(\n",
    "                file_symbols.keys(),\n",
    "                key=lambda s: self.symbol_importance.get(s, 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            file_map = []\n",
    "            file_token_count = self.estimate_token_count(filepath) + 10  # Base tokens for the file path\n",
    "            \n",
    "            for symbol in sorted_symbols:\n",
    "                symbol_info = file_symbols[symbol]\n",
    "                symbol_text = symbol_info[\"code_text\"]\n",
    "                symbol_tokens = self.estimate_token_count(symbol_text) + 5  # Extra tokens for formatting\n",
    "                \n",
    "                # Check if adding this symbol would exceed token budget\n",
    "                if total_tokens + file_token_count + symbol_tokens > self.max_tokens:\n",
    "                    # If we haven't added any symbols to this file yet, add at least the most important one\n",
    "                    if not file_map and symbol == sorted_symbols[0]:\n",
    "                        file_map.append(symbol_info)\n",
    "                        total_tokens += symbol_tokens\n",
    "                    break\n",
    "                \n",
    "                file_map.append(symbol_info)\n",
    "                file_token_count += symbol_tokens\n",
    "            \n",
    "            if file_map:\n",
    "                repo_map[filepath] = file_map\n",
    "                total_tokens += file_token_count\n",
    "            \n",
    "            # Stop adding files if we're near the token budget\n",
    "            if total_tokens >= self.max_tokens * 0.95:\n",
    "                break\n",
    "        \n",
    "        return repo_map\n",
    "    \n",
    "    def format_repo_map_for_ai(self, repo_map):\n",
    "        \"\"\"Format the repository map for AI consumption\"\"\"\n",
    "        formatted_map = []\n",
    "        \n",
    "        for filepath, symbols in repo_map.items():\n",
    "            file_section = [f\"{filepath}:\"]\n",
    "            \n",
    "            # Add indentation to show file structure\n",
    "            for symbol in symbols:\n",
    "                code_text = symbol[\"code_text\"]\n",
    "                # Add indentation and pipe character to every line\n",
    "                indented_code = \"\\n\".join(f\"│{line}\" for line in code_text.split(\"\\n\"))\n",
    "                file_section.append(indented_code)\n",
    "                file_section.append(\"⋮...\")  # Ellipsis to indicate there's more code in the file\n",
    "            \n",
    "            formatted_map.append(\"\\n\".join(file_section))\n",
    "        \n",
    "        return \"\\n\".join(formatted_map)\n",
    "    \n",
    "    def output_repo_map(self, repo_map, format_type=\"ai\", output_file=None):\n",
    "        \"\"\"Output the repository map in the specified format\"\"\"\n",
    "        if format_type == \"ai\":\n",
    "            formatted_map = self.format_repo_map_for_ai(repo_map)\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(formatted_map)\n",
    "            return formatted_map\n",
    "        elif format_type == \"json\":\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(repo_map, f, indent=2)\n",
    "            return json.dumps(repo_map, indent=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format type: {format_type}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # For notebook execution\n",
    "    from IPython.display import display\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    # Input widget for repository path\n",
    "    dir_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter the path to your Python repository',\n",
    "        description='Repository Path:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    display(dir_input)\n",
    "    \n",
    "    # Default repository path - adjust as needed\n",
    "    root_directory = r\"C:\\python\\erpnext\\erpnext\"\n",
    "    \n",
    "    # Maximum tokens for the repo map (adjust based on your AI system's limitations)\n",
    "    max_tokens = 1000000\n",
    "    \n",
    "    # Create and run the repo map generator\n",
    "    generator = RepoMapGenerator(root_directory, max_tokens)\n",
    "    repo_map = generator.process_repository()\n",
    "    \n",
    "    # Output the repo map in both formats\n",
    "    ai_format = generator.output_repo_map(repo_map, \"ai\", \"repo_map_ai.txt\")\n",
    "    json_format = generator.output_repo_map(repo_map, \"json\", \"repo_map.json\")\n",
    "    \n",
    "    print(\"\\nRepository map has been generated.\")\n",
    "    print(\"AI-friendly format saved to: repo_map_ai.txt\")\n",
    "    print(\"JSON format saved to: repo_map.json\")\n",
    "    \n",
    "    # Show a preview of the AI-friendly format\n",
    "    print(\"\\nPreview of the AI-friendly format:\")\n",
    "    preview_lines = ai_format.split(\"\\n\")[:20]\n",
    "    print(\"\\n\".join(preview_lines))\n",
    "    if len(preview_lines) < len(ai_format.split(\"\\n\")):\n",
    "        print(\"... (more content in repo_map_ai.txt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cbe5d3-3b2e-46f5-9de0-6b9d96d4a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee6da22f94b44f0874671f29527f528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Repository Path:', layout=Layout(width='80%'), placeholder='Enter the path to your…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ddc2a9be44d63b9b612ce55571969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Module Path:', layout=Layout(width='80%'), placeholder='Enter the path to specific…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf40f3be36b94b488f8e164ab2aed844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100000, continuous_update=False, description='Max Tokens:', max=1000000, min=10000, step=10000…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a3b30391f04caabe8938aa979ae41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='repo_map_ai.txt', description='AI Output:', layout=Layout(width='80%'), placeholder='Path for AI-f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e304ebd2e2ea4447a193192651dea65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='repo_map.json', description='JSON Output:', layout=Layout(width='80%'), placeholder='Path for JSON…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c101f97dbb4c4b10b1260d2cdc944bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Repo Map', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python Repository Map Generator for AI Processing\n",
    "# Optimized for processing specific modules within larger repositories\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional, Union, Set\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "class RepoMapGenerator:\n",
    "    def __init__(self, root_dir, module_dir=None, max_tokens=1000):\n",
    "        self.root_dir = os.path.abspath(root_dir)\n",
    "        self.module_dir = os.path.abspath(module_dir) if module_dir else self.root_dir\n",
    "        self.max_tokens = max_tokens\n",
    "        self.files_data = []\n",
    "        self.symbol_references = defaultdict(int)\n",
    "        self.file_dependencies = defaultdict(set)\n",
    "        self.symbol_definitions = {}  # filepath -> symbol -> line info\n",
    "        self.module_files = set()  # Files that are part of the target module\n",
    "        self.processed_files = set()  # Track files we've already processed\n",
    "        self.relevant_symbols = set()  # Symbols that are relevant to the module\n",
    "        \n",
    "    def process_repository(self):\n",
    "        \"\"\"Process the repository focusing on the specified module\"\"\"\n",
    "        print(f\"Processing repository: {self.root_dir}\")\n",
    "        print(f\"Target module: {self.module_dir}\")\n",
    "        \n",
    "        # First pass: process the target module files\n",
    "        self.process_module_files()\n",
    "        \n",
    "        # Second pass: identify and process related files from the references\n",
    "        self.process_related_files()\n",
    "        \n",
    "        # Third pass: compute importance scores\n",
    "        self.compute_symbol_importance()\n",
    "        \n",
    "        # Fourth pass: build the repo map\n",
    "        repo_map = self.build_repo_map()\n",
    "        \n",
    "        print(f\"\\nFinished processing. Map contains {len(repo_map)} files.\")\n",
    "        return repo_map\n",
    "    \n",
    "    def process_module_files(self):\n",
    "        \"\"\"First pass: process all Python files in the target module\"\"\"\n",
    "        for root, _, files in os.walk(self.module_dir):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".py\"):\n",
    "                    filepath = os.path.join(root, filename)\n",
    "                    rel_path = os.path.relpath(filepath, self.root_dir)\n",
    "                    \n",
    "                    print(f\"Processing module file: {rel_path}\", end=\"\\r\")\n",
    "                    self.module_files.add(filepath)\n",
    "                    file_data = self.parse_python_file(filepath)\n",
    "                    \n",
    "                    if file_data:\n",
    "                        self.files_data.append(file_data)\n",
    "                        self.extract_symbols_and_references(file_data)\n",
    "                        self.processed_files.add(filepath)\n",
    "                        \n",
    "                        # Collect all symbols defined in this module\n",
    "                        if filepath in self.symbol_definitions:\n",
    "                            for symbol in self.symbol_definitions[filepath]:\n",
    "                                self.relevant_symbols.add(symbol)\n",
    "    \n",
    "    def process_related_files(self):\n",
    "        \"\"\"Second pass: follow dependencies and references to process related files\"\"\"\n",
    "        # Start with the dependencies from module files\n",
    "        to_process = set()\n",
    "        for file_path in self.processed_files:\n",
    "            rel_path = os.path.relpath(file_path, self.root_dir)\n",
    "            deps = self.file_dependencies.get(rel_path, set())\n",
    "            to_process.update(deps)\n",
    "        \n",
    "        # Also collect referenced symbols that we don't have definitions for yet\n",
    "        referenced_symbols = set(self.symbol_references.keys())\n",
    "        \n",
    "        # Continue until we've processed all related files\n",
    "        iteration = 1\n",
    "        while to_process and iteration <= 3:  # Limit depth to prevent processing the entire repo\n",
    "            print(f\"\\nIteration {iteration}: Processing {len(to_process)} related dependencies\")\n",
    "            iteration += 1\n",
    "            \n",
    "            # Find Python files matching the dependency names\n",
    "            new_files = set()\n",
    "            for dep_name in to_process:\n",
    "                for root, _, files in os.walk(self.root_dir):\n",
    "                    for filename in files:\n",
    "                        if filename == f\"{dep_name}.py\" or filename.endswith(\".py\") and os.path.basename(root) == dep_name:\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            if filepath not in self.processed_files:\n",
    "                                new_files.add(filepath)\n",
    "            \n",
    "            # Also look for files that might define our referenced symbols\n",
    "            for root, _, files in os.walk(self.root_dir):\n",
    "                for filename in files:\n",
    "                    if not filename.endswith(\".py\"):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip if already processed\n",
    "                    filepath = os.path.join(root, filename)\n",
    "                    if filepath in self.processed_files or filepath in new_files:\n",
    "                        continue\n",
    "                    \n",
    "                    # Simple heuristic: check if the filename matches any part of our referenced symbols\n",
    "                    basename = os.path.splitext(filename)[0]\n",
    "                    for symbol in referenced_symbols:\n",
    "                        if basename.lower() in symbol.lower() or symbol.lower() in basename.lower():\n",
    "                            new_files.add(filepath)\n",
    "                            break\n",
    "            \n",
    "            # Process the new files\n",
    "            new_deps = set()\n",
    "            for filepath in new_files:\n",
    "                rel_path = os.path.relpath(filepath, self.root_dir)\n",
    "                print(f\"Processing related file: {rel_path}\", end=\"\\r\")\n",
    "                \n",
    "                file_data = self.parse_python_file(filepath)\n",
    "                if file_data:\n",
    "                    self.files_data.append(file_data)\n",
    "                    self.extract_symbols_and_references(file_data)\n",
    "                    self.processed_files.add(filepath)\n",
    "                    \n",
    "                    # Add symbols from this file to relevant symbols\n",
    "                    if filepath in self.symbol_definitions:\n",
    "                        for symbol in self.symbol_definitions[filepath]:\n",
    "                            self.relevant_symbols.add(symbol)\n",
    "                    \n",
    "                    # Collect new dependencies\n",
    "                    rel_path = file_data[\"rel_filepath\"]\n",
    "                    deps = self.file_dependencies.get(rel_path, set())\n",
    "                    new_deps.update(deps)\n",
    "            \n",
    "            # Update to_process with new dependencies, excluding ones we've already processed\n",
    "            to_process = new_deps - set(dep for dep in new_deps if any(\n",
    "                os.path.join(self.root_dir, f\"{dep}.py\") == f or \n",
    "                os.path.basename(os.path.dirname(f)) == dep \n",
    "                for f in self.processed_files\n",
    "            ))\n",
    "    \n",
    "    def parse_python_file(self, filepath: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a Python file using AST\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                source_code = file.read()\n",
    "                source_lines = source_code.splitlines()\n",
    "            \n",
    "            # Parse the source code into an AST\n",
    "            tree = ast.parse(source_code)\n",
    "            \n",
    "            # Extract file information\n",
    "            rel_path = os.path.relpath(filepath, self.root_dir)\n",
    "            \n",
    "            file_data = {\n",
    "                \"filepath\": filepath,\n",
    "                \"rel_filepath\": rel_path,\n",
    "                \"source_lines\": source_lines,\n",
    "                \"symbols\": self.extract_symbols(tree, source_lines),\n",
    "                \"imports\": self.extract_imports(tree),\n",
    "                \"references\": self.extract_references(tree)\n",
    "            }\n",
    "            \n",
    "            return file_data\n",
    "        \n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in {filepath}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {filepath}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_symbols(self, tree: ast.AST, source_lines: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract symbol definitions from AST\"\"\"\n",
    "        symbols = []\n",
    "        \n",
    "        for node in ast.iter_child_nodes(tree):\n",
    "            if isinstance(node, ast.ClassDef):\n",
    "                # Extract class definition\n",
    "                class_symbol = {\n",
    "                    \"type\": \"class\",\n",
    "                    \"name\": node.name,\n",
    "                    \"line_start\": node.lineno,\n",
    "                    \"line_end\": getattr(node, 'end_lineno', node.lineno),\n",
    "                    \"code_text\": self.get_definition_text(source_lines, node),\n",
    "                    \"methods\": []\n",
    "                }\n",
    "                \n",
    "                # Extract class methods\n",
    "                for item in node.body:\n",
    "                    if isinstance(item, ast.FunctionDef):\n",
    "                        method_symbol = {\n",
    "                            \"type\": \"method\",\n",
    "                            \"name\": f\"{node.name}.{item.name}\",\n",
    "                            \"method_name\": item.name,\n",
    "                            \"line_start\": item.lineno,\n",
    "                            \"line_end\": getattr(item, 'end_lineno', item.lineno),\n",
    "                            \"code_text\": self.get_definition_text(source_lines, item),\n",
    "                            \"args\": self.get_function_args(item)\n",
    "                        }\n",
    "                        class_symbol[\"methods\"].append(method_symbol)\n",
    "                \n",
    "                symbols.append(class_symbol)\n",
    "            \n",
    "            elif isinstance(node, ast.FunctionDef):\n",
    "                # Extract function definition\n",
    "                func_symbol = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"name\": node.name,\n",
    "                    \"line_start\": node.lineno,\n",
    "                    \"line_end\": getattr(node, 'end_lineno', node.lineno),\n",
    "                    \"code_text\": self.get_definition_text(source_lines, node),\n",
    "                    \"args\": self.get_function_args(node)\n",
    "                }\n",
    "                symbols.append(func_symbol)\n",
    "        \n",
    "        return symbols\n",
    "    \n",
    "    def get_definition_text(self, source_lines: List[str], node: Union[ast.ClassDef, ast.FunctionDef]) -> str:\n",
    "        \"\"\"Get the definition line(s) for a symbol\"\"\"\n",
    "        start_line = node.lineno - 1  # 0-indexed\n",
    "        \n",
    "        # For single-line functions or classes\n",
    "        if hasattr(node, 'end_lineno'):\n",
    "            if node.end_lineno - node.lineno <= 5:  # If definition is short (≤ 5 lines)\n",
    "                # Get full definition \n",
    "                return \"\\n\".join(source_lines[start_line:node.end_lineno])\n",
    "        \n",
    "        # For longer definitions or when end_lineno is not available\n",
    "        # Get just the signature/declaration\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            # For classes, include inheritance info\n",
    "            line = source_lines[start_line]\n",
    "            # If declaration continues to next lines, include them\n",
    "            current_line = start_line + 1\n",
    "            while current_line < len(source_lines) and ':' not in line:\n",
    "                line += \"\\n\" + source_lines[current_line]\n",
    "                current_line += 1\n",
    "            return line\n",
    "        \n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            # For functions, include the full signature\n",
    "            signature_lines = []\n",
    "            signature_lines.append(source_lines[start_line])\n",
    "            \n",
    "            # If parameter list continues to next lines\n",
    "            line = source_lines[start_line]\n",
    "            paren_count = line.count('(') - line.count(')')\n",
    "            current_line = start_line + 1\n",
    "            \n",
    "            # Keep adding lines until we have a balanced parenthesis count or hit a colon\n",
    "            while paren_count > 0 and current_line < len(source_lines):\n",
    "                next_line = source_lines[current_line]\n",
    "                signature_lines.append(next_line)\n",
    "                paren_count += next_line.count('(') - next_line.count(')')\n",
    "                \n",
    "                if ':' in next_line:\n",
    "                    break\n",
    "                    \n",
    "                current_line += 1\n",
    "            \n",
    "            return \"\\n\".join(signature_lines)\n",
    "    \n",
    "    def get_function_args(self, node: ast.FunctionDef) -> List[str]:\n",
    "        \"\"\"Extract function arguments\"\"\"\n",
    "        args = []\n",
    "        for arg in node.args.args:\n",
    "            arg_str = arg.arg\n",
    "            if arg.annotation:\n",
    "                if isinstance(arg.annotation, ast.Name):\n",
    "                    arg_str += f\": {arg.annotation.id}\"\n",
    "                elif isinstance(arg.annotation, ast.Attribute):\n",
    "                    arg_str += f\": {self.get_attribute_name(arg.annotation)}\"\n",
    "            args.append(arg_str)\n",
    "        \n",
    "        # Add *args if present\n",
    "        if node.args.vararg:\n",
    "            args.append(f\"*{node.args.vararg.arg}\")\n",
    "        \n",
    "        # Add **kwargs if present\n",
    "        if node.args.kwarg:\n",
    "            args.append(f\"**{node.args.kwarg.arg}\")\n",
    "            \n",
    "        return args\n",
    "    \n",
    "    def get_attribute_name(self, node: ast.Attribute) -> str:\n",
    "        \"\"\"Get the full name of an attribute node\"\"\"\n",
    "        if isinstance(node.value, ast.Name):\n",
    "            return f\"{node.value.id}.{node.attr}\"\n",
    "        elif isinstance(node.value, ast.Attribute):\n",
    "            return f\"{self.get_attribute_name(node.value)}.{node.attr}\"\n",
    "        return f\"?.{node.attr}\"\n",
    "    \n",
    "    def extract_imports(self, tree: ast.AST) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract import statements\"\"\"\n",
    "        imports = []\n",
    "        \n",
    "        for node in ast.iter_child_nodes(tree):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"type\": \"import\",\n",
    "                        \"name\": name.name,\n",
    "                        \"asname\": name.asname\n",
    "                    })\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                module = node.module or \"\"\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"type\": \"import_from\",\n",
    "                        \"module\": module,\n",
    "                        \"name\": name.name,\n",
    "                        \"asname\": name.asname\n",
    "                    })\n",
    "        \n",
    "        return imports\n",
    "    \n",
    "    def extract_references(self, tree: ast.AST) -> List[str]:\n",
    "        \"\"\"Extract symbol references from AST\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        class ReferenceVisitor(ast.NodeVisitor):\n",
    "            def __init__(self):\n",
    "                self.refs = []\n",
    "                \n",
    "            def visit_Name(self, node):\n",
    "                if isinstance(node.ctx, ast.Load):\n",
    "                    self.refs.append(node.id)\n",
    "                self.generic_visit(node)\n",
    "                \n",
    "            def visit_Attribute(self, node):\n",
    "                if isinstance(node.value, ast.Name):\n",
    "                    self.refs.append(f\"{node.value.id}.{node.attr}\")\n",
    "                self.generic_visit(node)\n",
    "        \n",
    "        visitor = ReferenceVisitor()\n",
    "        visitor.visit(tree)\n",
    "        return visitor.refs\n",
    "    \n",
    "    def extract_symbols_and_references(self, file_data: Dict[str, Any]):\n",
    "        \"\"\"Extract symbols and references from file data\"\"\"\n",
    "        filepath = file_data[\"rel_filepath\"]\n",
    "        \n",
    "        # Store symbol definitions\n",
    "        self.symbol_definitions[filepath] = {}\n",
    "        for symbol in file_data[\"symbols\"]:\n",
    "            if symbol[\"type\"] == \"class\":\n",
    "                self.symbol_definitions[filepath][symbol[\"name\"]] = {\n",
    "                    \"type\": \"class\",\n",
    "                    \"line_start\": symbol[\"line_start\"],\n",
    "                    \"code_text\": symbol[\"code_text\"]\n",
    "                }\n",
    "                \n",
    "                # Add methods\n",
    "                for method in symbol[\"methods\"]:\n",
    "                    self.symbol_definitions[filepath][method[\"name\"]] = {\n",
    "                        \"type\": \"method\",\n",
    "                        \"line_start\": method[\"line_start\"],\n",
    "                        \"code_text\": method[\"code_text\"],\n",
    "                        \"args\": method[\"args\"]\n",
    "                    }\n",
    "            \n",
    "            elif symbol[\"type\"] == \"function\":\n",
    "                self.symbol_definitions[filepath][symbol[\"name\"]] = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"line_start\": symbol[\"line_start\"],\n",
    "                    \"code_text\": symbol[\"code_text\"],\n",
    "                    \"args\": symbol[\"args\"]\n",
    "                }\n",
    "        \n",
    "        # Count references\n",
    "        for ref in file_data[\"references\"]:\n",
    "            self.symbol_references[ref] += 1\n",
    "        \n",
    "        # Track file dependencies based on imports\n",
    "        for imp in file_data[\"imports\"]:\n",
    "            if imp[\"type\"] == \"import\":\n",
    "                self.file_dependencies[filepath].add(imp[\"name\"].split(\".\")[0])\n",
    "            elif imp[\"type\"] == \"import_from\":\n",
    "                self.file_dependencies[filepath].add(imp[\"module\"].split(\".\")[0])\n",
    "    \n",
    "    def compute_symbol_importance(self):\n",
    "        \"\"\"Compute importance scores for symbols with focus on module relevance\"\"\"\n",
    "        self.symbol_importance = {}\n",
    "        \n",
    "        # Start with reference counts as base importance\n",
    "        for symbol, count in self.symbol_references.items():\n",
    "            self.symbol_importance[symbol] = count\n",
    "        \n",
    "        # Boost importance for symbols defined in our target module\n",
    "        for filepath in self.symbol_definitions:\n",
    "            # Check if this file is in our target module\n",
    "            abs_path = os.path.join(self.root_dir, filepath)\n",
    "            is_module_file = abs_path in self.module_files\n",
    "            \n",
    "            for symbol in self.symbol_definitions[filepath]:\n",
    "                # Boost module symbols significantly\n",
    "                if is_module_file:\n",
    "                    if symbol in self.symbol_importance:\n",
    "                        self.symbol_importance[symbol] += 10  # Higher boost for module symbols\n",
    "                    else:\n",
    "                        self.symbol_importance[symbol] = 10\n",
    "                \n",
    "                # Ensure all symbols have a base importance\n",
    "                elif symbol not in self.symbol_importance:\n",
    "                    self.symbol_importance[symbol] = 1\n",
    "        \n",
    "        # Add importance based on dependency graph\n",
    "        dependency_counts = Counter()\n",
    "        for file, deps in self.file_dependencies.items():\n",
    "            for dep in deps:\n",
    "                dependency_counts[dep] += 1\n",
    "        \n",
    "        # Find Python files that match dependency names and boost their symbols\n",
    "        for filepath in self.symbol_definitions:\n",
    "            basename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "            dirname = os.path.basename(os.path.dirname(os.path.join(self.root_dir, filepath)))\n",
    "            \n",
    "            boost = dependency_counts.get(basename, 0) + dependency_counts.get(dirname, 0)\n",
    "            \n",
    "            if boost > 0:\n",
    "                for symbol in self.symbol_definitions[filepath]:\n",
    "                    if symbol in self.symbol_importance:\n",
    "                        self.symbol_importance[symbol] += boost\n",
    "    \n",
    "    def estimate_token_count(self, text):\n",
    "        \"\"\"Estimate token count based on words and punctuation\"\"\"\n",
    "        # Simple token count estimation based on GPT tokenization heuristics\n",
    "        # This is a rough approximation\n",
    "        return len(re.findall(r'\\w+|[^\\w\\s]', text))\n",
    "    \n",
    "    def build_repo_map(self):\n",
    "        \"\"\"Build the repository map optimized for token count with focus on module relevance\"\"\"\n",
    "        repo_map = {}\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # Ensure module files are included first\n",
    "        module_filepaths = []\n",
    "        related_filepaths = []\n",
    "        \n",
    "        for filepath in self.symbol_definitions:\n",
    "            abs_path = os.path.join(self.root_dir, filepath)\n",
    "            if abs_path in self.module_files:\n",
    "                module_filepaths.append(filepath)\n",
    "            else:\n",
    "                related_filepaths.append(filepath)\n",
    "        \n",
    "        # Sort files by importance within their category\n",
    "        file_importance = {}\n",
    "        for filepath in self.symbol_definitions:\n",
    "            # Calculate file importance based on symbols it contains\n",
    "            importance = sum(\n",
    "                self.symbol_importance.get(symbol, 0)\n",
    "                for symbol in self.symbol_definitions[filepath]\n",
    "            )\n",
    "            file_importance[filepath] = importance\n",
    "        \n",
    "        sorted_module_files = sorted(module_filepaths, key=lambda f: file_importance[f], reverse=True)\n",
    "        sorted_related_files = sorted(related_filepaths, key=lambda f: file_importance[f], reverse=True)\n",
    "        \n",
    "        # Combine the lists with module files first\n",
    "        sorted_files = sorted_module_files + sorted_related_files\n",
    "        \n",
    "        # Build map with the most important files first\n",
    "        for filepath in sorted_files:\n",
    "            file_symbols = self.symbol_definitions[filepath]\n",
    "            \n",
    "            # Sort symbols by importance\n",
    "            sorted_symbols = sorted(\n",
    "                file_symbols.keys(),\n",
    "                key=lambda s: self.symbol_importance.get(s, 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            file_map = []\n",
    "            file_token_count = self.estimate_token_count(filepath) + 10  # Base tokens for the file path\n",
    "            \n",
    "            for symbol in sorted_symbols:\n",
    "                symbol_info = file_symbols[symbol]\n",
    "                symbol_text = symbol_info[\"code_text\"]\n",
    "                symbol_tokens = self.estimate_token_count(symbol_text) + 5  # Extra tokens for formatting\n",
    "                \n",
    "                # Check if adding this symbol would exceed token budget\n",
    "                if total_tokens + file_token_count + symbol_tokens > self.max_tokens:\n",
    "                    # If we haven't added any symbols to this file yet, add at least the most important one\n",
    "                    if not file_map and symbol == sorted_symbols[0]:\n",
    "                        file_map.append(symbol_info)\n",
    "                        total_tokens += symbol_tokens\n",
    "                    break\n",
    "                \n",
    "                file_map.append(symbol_info)\n",
    "                file_token_count += symbol_tokens\n",
    "            \n",
    "            if file_map:\n",
    "                repo_map[filepath] = file_map\n",
    "                total_tokens += file_token_count\n",
    "            \n",
    "            # Stop adding files if we're near the token budget\n",
    "            if total_tokens >= self.max_tokens * 0.95:\n",
    "                break\n",
    "        \n",
    "        return repo_map\n",
    "    \n",
    "    def format_repo_map_for_ai(self, repo_map):\n",
    "        \"\"\"Format the repository map for AI consumption\"\"\"\n",
    "        formatted_map = []\n",
    "        \n",
    "        # First add module files\n",
    "        module_files = []\n",
    "        related_files = []\n",
    "        \n",
    "        for filepath, symbols in repo_map.items():\n",
    "            abs_path = os.path.join(self.root_dir, filepath) \n",
    "            is_module_file = abs_path in self.module_files\n",
    "            \n",
    "            file_section = [f\"{filepath}:\"]\n",
    "            \n",
    "            # Add indentation to show file structure\n",
    "            for symbol in symbols:\n",
    "                code_text = symbol[\"code_text\"]\n",
    "                # Add indentation and pipe character to every line\n",
    "                indented_code = \"\\n\".join(f\"│{line}\" for line in code_text.split(\"\\n\"))\n",
    "                file_section.append(indented_code)\n",
    "                file_section.append(\"⋮...\")  # Ellipsis to indicate there's more code in the file\n",
    "            \n",
    "            formatted_section = \"\\n\".join(file_section)\n",
    "            \n",
    "            if is_module_file:\n",
    "                module_files.append(formatted_section)\n",
    "            else:\n",
    "                related_files.append(formatted_section)\n",
    "        \n",
    "        # Add module files first, then related files\n",
    "        formatted_map.extend(module_files)\n",
    "        \n",
    "        if module_files and related_files:\n",
    "            formatted_map.append(\"\\n--- Related Dependencies ---\\n\")\n",
    "        \n",
    "        formatted_map.extend(related_files)\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_map)\n",
    "    \n",
    "    def output_repo_map(self, repo_map, format_type=\"ai\", output_file=None):\n",
    "        \"\"\"Output the repository map in the specified format\"\"\"\n",
    "        if format_type == \"ai\":\n",
    "            formatted_map = self.format_repo_map_for_ai(repo_map)\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(formatted_map)\n",
    "            return formatted_map\n",
    "        elif format_type == \"json\":\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(repo_map, f, indent=2)\n",
    "            return json.dumps(repo_map, indent=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format type: {format_type}\")\n",
    "\n",
    "# For Jupyter notebook execution - no argparse required\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Input widgets for repository path and module path\n",
    "repo_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter the path to your Python repository',\n",
    "    description='Repository Path:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "module_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter the path to specific module (leave empty for entire repo)',\n",
    "    description='Module Path:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "max_tokens_input = widgets.IntSlider(\n",
    "    value=100000,\n",
    "    min=10000,\n",
    "    max=1000000,\n",
    "    step=10000,\n",
    "    description='Max Tokens:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "output_ai_input = widgets.Text(\n",
    "    value='repo_map_ai.txt',\n",
    "    placeholder='Path for AI-friendly output file',\n",
    "    description='AI Output:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "output_json_input = widgets.Text(\n",
    "    value='repo_map.json',\n",
    "    placeholder='Path for JSON output file',\n",
    "    description='JSON Output:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "display(repo_input)\n",
    "display(module_input)\n",
    "display(max_tokens_input)\n",
    "display(output_ai_input)\n",
    "display(output_json_input)\n",
    "\n",
    "def run_generator(b):\n",
    "    repo_dir = repo_input.value\n",
    "    module_dir = module_input.value\n",
    "    max_tokens = max_tokens_input.value\n",
    "    output_ai = output_ai_input.value\n",
    "    output_json = output_json_input.value\n",
    "    \n",
    "    if not repo_dir:\n",
    "        print(\"Please enter a repository path\")\n",
    "        return\n",
    "    \n",
    "    if module_dir and not os.path.isdir(module_dir):\n",
    "        print(f\"Module directory {module_dir} not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing repo: {repo_dir}\")\n",
    "    print(f\"Target module: {module_dir or 'Entire repository'}\")\n",
    "    print(f\"Max tokens: {max_tokens}\")\n",
    "    \n",
    "    generator = RepoMapGenerator(repo_dir, module_dir, max_tokens)\n",
    "    repo_map = generator.process_repository()\n",
    "    \n",
    "    ai_format = generator.output_repo_map(repo_map, \"ai\", output_ai)\n",
    "    json_format = generator.output_repo_map(repo_map, \"json\", output_json)\n",
    "    \n",
    "    print(\"\\nRepository map has been generated.\")\n",
    "    print(f\"AI-friendly format saved to: {output_ai}\")\n",
    "    print(f\"JSON format saved to: {output_json}\")\n",
    "    \n",
    "    # Show a preview\n",
    "    print(\"\\nPreview of the AI-friendly format:\")\n",
    "    preview_lines = ai_format.split(\"\\n\")[:20]\n",
    "    print(\"\\n\".join(preview_lines))\n",
    "    if len(preview_lines) < len(ai_format.split(\"\\n\")):\n",
    "        print(\"... (more content in the output file)\")\n",
    "\n",
    "run_button = widgets.Button(description=\"Generate Repo Map\")\n",
    "run_button.on_click(run_generator)\n",
    "display(run_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a850e2c-9cc2-4549-a2ad-f341195e102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "REPO_PATH = r\"C:\\python\\frappe\"\n",
    "OUTPUT_FILE = \"frappe_tree.txt\"\n",
    "\n",
    "def get_definitions(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            tree = ast.parse(f.read(), filename=filepath)\n",
    "        classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n",
    "        funcs = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]\n",
    "        return classes, funcs\n",
    "    except Exception:\n",
    "        return [], []\n",
    "\n",
    "def build_tree(startpath):\n",
    "    lines = []\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent_str = '│   ' * level + '├── '\n",
    "        lines.append(f\"{indent_str}{os.path.basename(root)}/\")\n",
    "        sub_indent = '│   ' * (level + 1)\n",
    "        for f in sorted(files):\n",
    "            if f.endswith('.py'):\n",
    "                file_path = os.path.join(root, f)\n",
    "                lines.append(f\"{sub_indent}├── {f}\")\n",
    "                classes, funcs = get_definitions(file_path)\n",
    "                for cls in classes:\n",
    "                    lines.append(f\"{sub_indent}│   ├── class {cls}\")\n",
    "                for func in funcs:\n",
    "                    lines.append(f\"{sub_indent}│   ├── def {func}()\")\n",
    "    return lines\n",
    "\n",
    "# Run and save output\n",
    "tree_lines = build_tree(REPO_PATH)\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:\n",
    "    out_file.write('\\n'.join(tree_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50a6c4-196e-49dc-b922-d68d6eea7457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
